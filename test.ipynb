{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "import torch\n",
    "import math\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "from torch import nn, optim\n",
    "\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from models.UNet import UNet\n",
    "from models.VisionTransformer import VisionTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dynamically select device\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = {\n",
    "    'in_channels': 3,\n",
    "    'out_channels': 3,\n",
    "    'num_layers': 6,\n",
    "    'time_steps': 50\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]) if model_config['in_channels'] == 3 else transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "])\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=transform),\n",
    "    batch_size=1,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "# dataloader = torch.utils.data.DataLoader(\n",
    "#     datasets.MNIST(root=\"./data\", train=True, download=False, transform=transform),\n",
    "#     batch_size=1,\n",
    "#     shuffle=True,\n",
    "# )\n",
    "\n",
    "# dataloader = torch.utils.data.DataLoader(\n",
    "#     datasets.StanfordCars(root=\"./data\", split='train', download=False, transform=transform),\n",
    "#     batch_size=1,\n",
    "#     shuffle=True,\n",
    "# )\n",
    "\n",
    "# dataloader = torch.utils.data.DataLoader(\n",
    "#     datasets.CelebA(root=\"./data\", split='train', download=False, transform=transform),\n",
    "#     batch_size=1,\n",
    "#     shuffle=True,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = next(iter(dataloader))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load trained model\n",
    "model_path = \"/Users/josh/Documents/GitHub/image-diffusion/checkpoints/diffusion-image-model/dim-2026_01_06_17_01_epoch99_end.pth\"\n",
    "chkpt = torch.load(model_path, weights_only=False, map_location=torch.device(device))\n",
    "\n",
    "# get model configuration\n",
    "model_config = chkpt['model_config']\n",
    "train_config = chkpt['train_config']\n",
    "\n",
    "model = UNet(\n",
    "    in_channels=model_config['in_channels'],\n",
    "    out_channels=model_config['out_channels'],\n",
    "    channels=model_config['channels'],\n",
    "    scales=model_config['scales'],\n",
    "    attentions=model_config['attentions'],\n",
    "    time_steps=model_config['time_steps'],\n",
    ").to(device)\n",
    "\n",
    "# model = VisionTransformer(\n",
    "#     patch_size=model_config['patch_size'],\n",
    "#     in_channels=model_config['in_channels'],\n",
    "#     out_channels=model_config['out_channels'],\n",
    "#     embed_dim=model_config['embed_dim'],\n",
    "#     num_layers=model_config['num_layers'],\n",
    "#     num_heads=model_config['num_heads'],\n",
    "#     time_steps=model_config['time_steps'],\n",
    "# ).to(device)\n",
    "\n",
    "model.load_state_dict(chkpt['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNet(\n",
    "    in_channels=model_config['in_channels'],\n",
    "    out_channels=model_config['out_channels'],\n",
    "    num_layers=model_config['num_layers'],\n",
    "    time_steps=model_config['time_steps'],\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "# dry run\n",
    "x = torch.randn(32, 3, 128, 128).to(device)\n",
    "t = torch.randint(0, model_config['time_steps'], (32,)).to(device)\n",
    "y = model(x, t)\n",
    "print(y.shape) # should be [32, 3, 32, 32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# diffusion scheduler\n",
    "beta = torch.linspace(1e-4, 0.02, model_config['time_steps'], requires_grad=False).to(device)\n",
    "alpha = 1.0 - beta\n",
    "alpha_hat = torch.cumprod(alpha, dim=0).requires_grad_(False).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.imshow((example * 0.5 + 0.5).squeeze(0).permute(1, 2, 0).cpu().numpy(), vmin=0, vmax=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = example.to(device)\n",
    "t = 10\n",
    "noise = torch.randn(batch.size(), device=batch.device, dtype=batch.dtype).to(device)\n",
    "diffuse_batch = math.sqrt(alpha_hat[t]) * batch + math.sqrt(1 - alpha_hat[t]) * noise\n",
    "\n",
    "plt.imshow((diffuse_batch * 0.5 + 0.5).squeeze(0).permute(1, 2, 0).cpu().numpy(), vmin=0, vmax=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = diffuse_batch\n",
    "pred_noise = model(diffuse_batch, t)\n",
    "\n",
    "if t > 0:\n",
    "    noise = torch.randn_like(x)\n",
    "else:\n",
    "    noise = 0\n",
    "\n",
    "x = (1 / torch.sqrt(alpha[t])) * (\n",
    "    x - (beta[t] / torch.sqrt(1 - alpha_hat[t])) * pred_noise\n",
    ") + torch.sqrt(beta[t]) * noise\n",
    "\n",
    "plt.imshow((x * 0.5 + 0.5).squeeze(0).permute(1, 2, 0).cpu().detach().numpy(), vmin=0, vmax=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = diffuse_batch - math.sqrt(1 - alpha_hat[t]) * model(diffuse_batch, t)\n",
    "image /= math.sqrt(alpha_hat[t])\n",
    "plt.imshow((image * 0.5 + 0.5).squeeze(0).permute(1, 2, 0).cpu().detach().numpy(), vmin=0, vmax=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.mps.empty_cache()\n",
    "x = torch.randn(1, model_config['in_channels'], train_config['image_size'], train_config['image_size']).to(device)\n",
    "\n",
    "for t in reversed(range(0, model_config['time_steps'])):\n",
    "    # predict noise\n",
    "    pred_noise = model(x, t)\n",
    "    x = (1 / torch.sqrt(alpha[t])) * (x - (beta[t] / torch.sqrt(1 - alpha_hat[t])) * pred_noise)\n",
    "\n",
    "    # add noise up to final generation\n",
    "    if t > 0:\n",
    "        x = x + torch.sqrt(beta[t]) * torch.randn_like(x).to(device)\n",
    "\n",
    "plt.imshow((x * 0.5 + 0.5).squeeze(0).permute(1, 2, 0).cpu().detach().numpy(), vmin=0, vmax=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-hw2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
