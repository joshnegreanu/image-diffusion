{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/josh/miniconda3/envs/nlp-hw2/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "import torch\n",
    "import math\n",
    "import pandas as pd\n",
    "import random\n",
    "import wandb\n",
    "\n",
    "# import deepinv\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from torch import nn, optim\n",
    "from torch.optim.lr_scheduler import LinearLR, CosineAnnealingLR, SequentialLR\n",
    "from torch.utils.data import DataLoader, Dataset, ConcatDataset\n",
    "\n",
    "from datasets import load_dataset\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from models.UNet import UNet\n",
    "from models.VisionTransformer import VisionTransformer\n",
    "\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dynamically select device\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "# dataloader = torch.utils.data.DataLoader(\n",
    "#     datasets.MNIST(root=\"./data\", train=True, download=True, transform=transform),\n",
    "#     batch_size=1,\n",
    "#     shuffle=True,\n",
    "# )\n",
    "\n",
    "# dataloader = torch.utils.data.DataLoader(\n",
    "#     datasets.StanfordCars(root=\"./data\", split='train', download=False, transform=transform),\n",
    "#     batch_size=1,\n",
    "#     shuffle=True,\n",
    "# )\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    datasets.CelebA(root=\"./data\", split='train', download=False, transform=transform),\n",
    "    batch_size=1,\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = next(iter(dataloader))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load trained model\n",
    "model_path = \"/Users/josh/Documents/GitHub/image-diffusion/checkpoints/diffusion-image-model/dim-2026_01_05_15_01_epoch129_end.pth\"\n",
    "chkpt = torch.load(model_path, weights_only=False, map_location=torch.device(device))\n",
    "\n",
    "# get model configuration\n",
    "model_config = chkpt['model_config']\n",
    "train_config = chkpt['train_config']\n",
    "\n",
    "model = UNet(\n",
    "    in_channels=model_config['in_channels'],\n",
    "    out_channels=model_config['out_channels'],\n",
    "    channels=model_config['channels'],\n",
    "    scales=model_config['scales'],\n",
    "    attentions=model_config['attentions'],\n",
    "    time_steps=model_config['time_steps'],\n",
    ").to(device)\n",
    "model.load_state_dict(chkpt['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VisionTransformer(\n",
    "    patch_size=4,\n",
    "    in_channels=3,\n",
    "    out_channels=3,\n",
    "    embed_dim=256,\n",
    "    num_layers=4,\n",
    "    num_heads=4\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_config)\n",
    "print(train_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = deepinv.models.DiffUNet(\n",
    "#     in_channels=1, out_channels=1, pretrained=None\n",
    "# ).to(device)\n",
    "\n",
    "# model.load_state_dict(chkpt['model_state_dict'])\n",
    "# model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "# dry run\n",
    "x = torch.randn(32, 3, 32, 32).to(device)\n",
    "y = model(x)\n",
    "print(y.shape) # should be [32, 3, 32, 32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# diffusion scheduler\n",
    "beta = torch.linspace(1e-4, 0.02, model_config['time_steps'], requires_grad=False).to(device)\n",
    "alpha = 1.0 - beta\n",
    "alpha_hat = torch.cumprod(alpha, dim=0).requires_grad_(False).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow((example * 0.5 + 0.5).squeeze(0).permute(1, 2, 0).cpu().numpy(), vmin=-1, vmax=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = example.to(device)\n",
    "t = 10\n",
    "noise = torch.randn(batch.size(), device=batch.device, dtype=batch.dtype).to(device)\n",
    "diffuse_batch = math.sqrt(alpha_hat[t]) * batch + math.sqrt(1 - alpha_hat[t]) * noise\n",
    "\n",
    "plt.imshow((diffuse_batch * 0.5 + 0.5).squeeze(0).permute(1, 2, 0).cpu().numpy(), vmin=-1, vmax=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = diffuse_batch\n",
    "pred_noise = model(diffuse_batch, t)\n",
    "\n",
    "if t > 0:\n",
    "    noise = torch.randn_like(x)\n",
    "else:\n",
    "    noise = 0\n",
    "\n",
    "x = (1 / torch.sqrt(alpha[t])) * (\n",
    "    x - (beta[t] / torch.sqrt(1 - alpha_hat[t])) * pred_noise\n",
    ") + torch.sqrt(beta[t]) * noise\n",
    "\n",
    "plt.imshow((x * 0.5 + 0.5).squeeze(0).permute(1, 2, 0).cpu().detach().numpy(), vmin=0, vmax=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = diffuse_batch - math.sqrt(1 - alpha_hat[t]) * model(diffuse_batch, t)\n",
    "image /= math.sqrt(alpha_hat[t])\n",
    "plt.imshow((image * 0.5 + 0.5).squeeze(0).permute(1, 2, 0).cpu().detach().numpy(), vmin=0, vmax=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.mps.empty_cache()\n",
    "x = torch.randn(1, model_config['in_channels'], train_config['image_size'], train_config['image_size']).to(device)\n",
    "\n",
    "for t in reversed(range(0, model_config['time_steps'])):\n",
    "    # predict noise\n",
    "    pred_noise = model(x, t)\n",
    "    x = (1 / torch.sqrt(alpha[t])) * (x - (beta[t] / torch.sqrt(1 - alpha_hat[t])) * pred_noise)\n",
    "\n",
    "    # add noise up to final generation\n",
    "    if t > 0:\n",
    "        x = x + torch.sqrt(beta[t]) * torch.randn_like(x).to(device)\n",
    "\n",
    "plt.imshow((x * 0.5 + 0.5).squeeze(0).permute(1, 2, 0).cpu().detach().numpy(), vmin=-1, vmax=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-hw2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
